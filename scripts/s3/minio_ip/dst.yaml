type: S3

access-key: '${DSTDB_ARC_USER}'
secret-key: '${DSTDB_ARC_PW}'


# if IP is used, then bucket name will not be used
bucket: '${DSTDB_ARC_USER}'
endpoint:
  service-endpoint: 'http://$(getent hosts ${DSTDB_HOST} | head -n 1 | awk '{print $1}'):${DSTDB_PORT}' 

# this must match mc create bucket/name from dst.init.roo.sh script
# cannot trailing / in the name
root: '${DSTDB_ARC_USER}/${DSTDB_DB}'

#directory where CSV files will be staged before uploading to S3
stage:
  type: SHARED_FS
  root-dir: '${CFG_DIR}/stage' 
file-format: CSV  # CSV | JSON

max-connections: 5
max-retries: 1
  
